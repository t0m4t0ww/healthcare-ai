# backend/app/services/gemini_service.py
"""
Google Gemini API Service - High Intelligence Configuration
S·ª≠ d·ª•ng: google-genai SDK (Latest)

C·∫•u h√¨nh: ∆Øu ti√™n GEMINI 2.5 PRO.
T√≠nh nƒÉng:
- Auto Fallback: N·∫øu 2.5 ch∆∞a c√≥, t·ª± d√πng 1.5 Pro.
- Smart Context: Nh·ªõ 50 l∆∞·ª£t chat g·∫ßn nh·∫•t.
- Retry: T·ª± ƒë·ªông th·ª≠ l·∫°i khi r·ªõt m·∫°ng.
- H·ªó tr·ª£ c·∫£ Streaming v√† One-shot.
"""
from __future__ import annotations
import os
import time
import socket
from typing import Optional, List, Callable

from google import genai
from google.genai import types
from bson import ObjectId

# =======================
# 1. C·∫§U H√åNH MODEL
# =======================
# ∆Øu ti√™n model b·∫°n mu·ªën d√πng
TARGET_MODEL = "gemini-2.5-pro"

# Danh s√°ch d·ª± ph√≤ng (Backup)
FALLBACK_MODELS = [
    TARGET_MODEL,           # ∆Øu ti√™n 1
    "gemini-1.5-pro",       # ∆Øu ti√™n 2
    "gemini-1.5-pro-002",   # ∆Øu ti√™n 3
    "gemini-2.0-flash-exp", # ∆Øu ti√™n 4
]

socket.setdefaulttimeout(30)

# =======================
# 2. KH·ªûI T·∫†O CLIENT
# =======================
def _load_api_key() -> str:
    key = (os.getenv("GEMINI_API_KEY") or os.getenv("GOOGLE_API_KEY") or "").strip()
    if not key:
        print("‚ùå CRITICAL: Thi·∫øu GEMINI_API_KEY trong file .env")
        return ""
    return key

def _build_client() -> Optional[genai.Client]:
    api_key = _load_api_key()
    if not api_key: return None
    
    print(f"üåê Kh·ªüi ƒë·ªông Gemini Service [Target: {TARGET_MODEL}]...")
    for attempt in range(1, 4):
        try:
            client = genai.Client(api_key=api_key)
            print("‚úÖ K·∫øt n·ªëi Google AI th√†nh c√¥ng.")
            return client
        except Exception as e:
            print(f"‚ö†Ô∏è L·ªói k·∫øt n·ªëi l·∫ßn {attempt}: {e}")
            time.sleep(1)
            
    print("‚ùå KH√îNG TH·ªÇ K·∫æT N·ªêI GEMINI API.")
    return None

try:
    client: Optional[genai.Client] = _build_client()
except:
    client = None

# Cache session trong RAM
chat_sessions: dict[str, any] = {}

# =======================
# 3. C√ÅC H√ÄM C·∫§U H√åNH
# =======================
def get_generation_config(temperature: float = 0.3, max_tokens: int = 2048):
    return types.GenerateContentConfig(
        temperature=temperature,
        top_p=0.95,
        max_output_tokens=max_tokens,
    )

def get_safety_settings():
    return [
        types.SafetySetting(category="HARM_CATEGORY_HARASSMENT", threshold="BLOCK_MEDIUM_AND_ABOVE"),
        types.SafetySetting(category="HARM_CATEGORY_HATE_SPEECH", threshold="BLOCK_MEDIUM_AND_ABOVE"),
        types.SafetySetting(category="HARM_CATEGORY_DANGEROUS_CONTENT", threshold="BLOCK_MEDIUM_AND_ABOVE"),
    ]

# =======================
# 4. LOGIC RETRY & FALLBACK
# =======================
RETRY_STATUS = {408, 409, 429, 500, 502, 503, 504}

def _with_backoff(call_fn: Callable[[str], any], *, attempts: int = 3):
    """Th·ª≠ g·ªçi API v·ªõi c∆° ch·∫ø ƒë·ªïi model n·∫øu l·ªói."""
    last_exc = None
    model_idx = 0
    
    for attempt in range(1, attempts + 1):
        current_model = FALLBACK_MODELS[model_idx]
        try:
            return call_fn(current_model)
        except Exception as e:
            last_exc = e
            err_msg = str(e).lower()
            
            # L·ªói model kh√¥ng t·ªìn t·∫°i -> ƒê·ªïi model
            if "404" in err_msg or "not found" in err_msg or "400" in err_msg:
                print(f"‚ö†Ô∏è Model '{current_model}' l·ªói/kh√¥ng c√≥. Fallback...")
                if model_idx + 1 < len(FALLBACK_MODELS):
                    model_idx += 1
                    continue
                else:
                    break
            
            # L·ªói m·∫°ng -> Retry
            if any(str(c) in err_msg for c in RETRY_STATUS):
                time.sleep(1 * (2 ** (attempt - 1)))
                continue
                
            break
            
    print(f"‚ùå API Failed: {last_exc}")
    raise last_exc

# =======================
# 5. QU·∫¢N L√ù SESSION
# =======================
def get_or_create_chat_session(conversation_id: str, system_prompt: str = None):
    if not client: return None
    if conversation_id in chat_sessions: return chat_sessions[conversation_id]

    history_sdk = []
    try:
        from app.extensions import mongo_db
        msgs = list(mongo_db.messages.find({"conversation_id": ObjectId(conversation_id)})
                    .sort("created_at", -1).limit(50))
        msgs.reverse()

        for m in msgs:
            role = "model" if (m.get("sender") or m.get("role")) in ["ai", "model"] else "user"
            text = m.get("text", "").strip()
            if text:
                history_sdk.append(types.Content(role=role, parts=[types.Part(text=text)]))
    except Exception:
        pass

    try:
        config = get_generation_config()
        if system_prompt: config.system_instruction = system_prompt
        
        # Init v·ªõi model ƒë·∫ßu ti√™n, n·∫øu l·ªói fallback s·∫Ω handle l√∫c send
        chat = client.chats.create(
            model=FALLBACK_MODELS[0], 
            config=config, 
            history=history_sdk
        )
        chat_sessions[conversation_id] = chat
        return chat
    except:
        # Fallback init
        try:
            chat = client.chats.create(model=FALLBACK_MODELS[1], config=config, history=history_sdk)
            chat_sessions[conversation_id] = chat
            return chat
        except:
            return None

def clear_chat_session(conversation_id: str):
    if conversation_id in chat_sessions: del chat_sessions[conversation_id]

# =======================
# 6. H√ÄM CH√çNH (STREAMING & ONE-SHOT)
# =======================

def gemini_chat_streaming(conversation_id: str, user_prompt: str, system: str = None) -> str:
    """Chat c√≥ nh·ªõ context (Streaming)"""
    if not user_prompt: return ""
    chat = get_or_create_chat_session(conversation_id, system_prompt=system)
    if not chat: return "‚ö†Ô∏è L·ªói k·∫øt n·ªëi AI."

    try:
        def _send(model_name_unused): 
            return chat.send_message_stream(user_prompt)

        resp_text = ""
        stream = _with_backoff(_send, attempts=3)
        for chunk in stream:
            if chunk.text: resp_text += chunk.text
            clean_text = resp_text.replace("**", "").replace("##", "").strip()
        return resp_text.strip()
    except Exception:
        return "Xin l·ªói, h·ªá th·ªëng ƒëang b·∫≠n."

def gemini_chat(
    user_prompt: str,
    system: str = None,
    history: list = None,
    temperature: float = 0.5,
    max_tokens: int = 1024
) -> str:
    """
    Chat 1 l·∫ßn (One-shot), kh√¥ng d√πng session, kh√¥ng streaming.
    D√πng ƒë·ªÉ t·∫°o g·ª£i √Ω c√¢u h·ªèi (Suggestions).
    """
    if not client: return ""

    # Build contents
    contents = []
    if history:
        # Map history dict to types.Content if needed (ƒë∆°n gi·∫£n h√≥a)
        pass 
    contents.append(types.Content(role="user", parts=[types.Part(text=user_prompt)]))

    def _do_generate(model_name):
        config = get_generation_config(temperature=temperature, max_tokens=max_tokens)
        if system: config.system_instruction = system
        
        return client.models.generate_content(
            model=model_name,
            contents=contents,
            config=config,
            safety_settings=get_safety_settings()
        )

    try:
        resp = _with_backoff(_do_generate, attempts=3)
        return resp.text.strip() if resp and resp.text else ""
    except Exception as e:
        print(f"One-shot Error: {e}")
        return ""

def analyze_xray_with_context(xray_findings: str, patient_info: str = "") -> str:
    """Ph√¢n t√≠ch X-quang chuy√™n s√¢u"""
    prompt = f"""
    PH√ÇN T√çCH CHUY√äN S√ÇU (Y√™u c·∫ßu ƒë·ªô ch√≠nh x√°c cao nh·∫•t):
    D·ªØ li·ªáu X-quang: {xray_findings}
    D·ªØ li·ªáu l√¢m s√†ng: {patient_info}
    Y√™u c·∫ßu: Ch·∫©n ƒëo√°n h√¨nh ·∫£nh, ch·∫©n ƒëo√°n ph√¢n bi·ªát, h∆∞·ªõng x·ª≠ tr√≠.
    """
    if not client: return "L·ªói k·∫øt n·ªëi."

    def _do_gen(model_name):
        return client.models.generate_content(
            model=model_name,
            contents=prompt,
            config=get_generation_config(temperature=0.2),
            safety_settings=get_safety_settings()
        )

    try:
        resp = _with_backoff(_do_gen, attempts=3)
        return resp.text.strip() if resp else "Kh√¥ng c√≥ k·∫øt qu·∫£."
    except Exception as e:
        return f"Kh√¥ng th·ªÉ ph√¢n t√≠ch: {str(e)}"

# =======================
# EXPORTS
# =======================
__all__ = [
    "gemini_chat_streaming", 
    "gemini_chat",  # <-- ƒê√£ th√™m h√†m n√†y v√†o exports
    "analyze_xray_with_context", 
    "clear_chat_session"
]